2/18
--
Surprised I didn't have a notes file in this repo but here's one.
-
I have a deep learning (CNN + SGD Nueral Net) example up and running in
a different repo. It requires images placed in each of:
    * training (1000)
    * evaluation (1000)
    * validation (1000)

So I need to make a new interim function in make_dataset, maybe something
like generate_three_data_splits, that creates those in interim.

From there it's a matter of making sure they're named properly (fake, real) with
a prefix. Let me check that one class svm can be batch learned and that
it can take neagtive examples....

okay, the one class i was aware of does not but there is a sgd one class (linear)
and, also, I should look at the reference example

Okay, so the reference example, here: https://hackernoon.com/one-class-classification-for-images-with-deep-features-be890c43455d
Doesn't seem to really take in account negativer data except isolation forest
has a contamination parameter

Let me double check what the CNN does
okay so the CNN needs data laid out in training, evaluation, and testing
and generates features on that into output
so it should look liek this:

make_dataset has a new function, generate_three_data_splits
this function creates a 
training set of only real kente images
a validation set of some real and some fake kente
a testing set of some real and more/some fake kente
---
okay so the quickest way to this is to modify interim/main not to delete (only main should),
copy out real to trainnig, and a mix otherwise to validation, testing

Then I can run features as normal

TheN I can run whichever/whatever of the above (even do parameter grid search if I care to)
k, let's do the first part...
-
note, I'm wiping the virtualenv,
doing
```
pipenv shell
pip install -r requirements.txt
```

which works (but is pretty ugly)
--

ran with

python src/data/make_dataset.py  makeinterim --seed 0 --number_per_real 325 --number_per_fake 125 --width 233 --height 233 --target_width 233 --target_height 233 --xrotation 20 --yrotation 30 --zrotation 3 -i ./data/raw/ -o ./interim/
-

okay, so now ./data/processed/ has
training has    2525 real
validation has  687 (500 real, 187 fake)
evaluation has     688 (500 real, 188 fake)

I should run the features generator and then go to bed.
----
2/19

okay, had to fix a bug in the feature genreator to work with kente related paths

What I'll do now is start running the SGD ... hmm i can't because train 
explicitly has no fake examples. I guess I can copy out 88 fake from testing
just to get the thing running and I can email out an accuracy point?
okay, moved

now let's run train.py

--
getting 73% ish

--

3/8
Wanting to do a stratified group split, as in
https://stackoverflow.com/questions/56872664/complex-dataset-split-stratifiedgroupshufflesplit

---
3/9

There's something off about the file counts

In [462]: len(list(chain.from_iterable((validation_indices, training_indices))))                                                                     
Out[462]: 1370

In [463]: len(list(interim_directory.glob('*.jpg')))                                                                                                 
Out[463]: 1984

but the indices are indirectly derived from the interim_dir list

So, that's actualy okay, since the gap is actually the evaluation set
but I think I need to store the whole file path and just go off of that,
when I indirectly link the glob() and the index things go awry
--
so the issue is that the trianing, validation indices include the test group,

In [492]: sampling_frame.loc[validation_indices].group.unique()                                                                                      
Out[492]: 
array(['fake_002', 'real_eglash', 'fake_001', 'real_philadelphia_unk',
       'real_spurlock_illinois_2011.05.0925.02',
       'real_fowler_women_X98-16-2_front__studio',
       'real_The_Met_1972.56.1_d1', 'real_miami_pinwheel_close_up',
       'real_DIA_1990.25_o2', 'real_The_Met_hb_1993.384.2',
       'real_The_Met_1972.56.1_d2', 'fake_003', 'real_miami_b',
       'real_philadelphia_museum_2001-170-1a-pma', 'real_UNL_quilt',
       'real_high_2004.166KenteCloth_o2', 'real_miami_a'], dtype=object)

In [493]: sampling_frame.loc[training_indices].group.unique()                                                                                        
Out[493]: 
array(['real_The_Met_1972.56.1_d2', 'fake_001', 'fake_002', 'fake_003',
       'real_high_2004.166KenteCloth_o2', 'real_miami_a',
       'real_miami_pinwheel_close_up', 'real_UNL_quilt',
       'real_DIA_1990.25_o2', 'real_fowler_women_X98-16-2_front__studio',
       'real_philadelphia_museum_2001-170-1a-pma',
       'real_philadelphia_unk', 'real_miami_b',
       'real_The_Met_1972.56.1_d1', 'real_eglash',
       'real_spurlock_illinois_2011.05.0925.02',
       'real_The_Met_hb_1993.384.2'], dtype=object)


even though we have
    validation_indices, training_indices =\
            next(
                StratifiedShuffleSplit(random_state=42,
                                       n_splits=1,
                                       test_size=0.5).split(
                                       sampling_frame.query('group not in @test_groups')\
                                                     .group,
                                       sampling_frame.query('group not in @test_groups')\
                                                     .y)
            )

and 

In [487]: test_groups                                                                                                                                
Out[487]: 
{'fake_001',
 'real_The_Met_1972.56.1_d2',
 'real_UNL_quilt',
 'real_fowler_women_X98-16-2_front__studio',
 'real_miami_a'}

I wouldn't expect fake_001 to show up

-----
yeah,
see


In [498]: test_groups                                                                                                                                
Out[498]: 
{'fake_001',
 'real_The_Met_1972.56.1_d2',
 'real_UNL_quilt',
 'real_fowler_women_X98-16-2_front__studio',
 'real_miami_a'}

In [499]:     validation_indices, training_indices =\ 
     ...:             next( 
     ...:                 StratifiedShuffleSplit(random_state=42, 
     ...:                                        n_splits=1, 
     ...:                                        test_size=0.5).split( 
     ...:                                        sampling_frame.query('group not in @test_groups')\ 
     ...:                                                      .group, 
     ...:                                        sampling_frame.query('group not in @test_groups')\ 
     ...:                                                      .y) 
     ...:             )                                                                                                                              


In [501]: sampling_frame.loc[training_indices].group.unique()                                                                                        
Out[501]: 
array(['real_The_Met_1972.56.1_d2', 'fake_001', 'fake_002', 'fake_003',
       'real_high_2004.166KenteCloth_o2', 'real_miami_a',
       'real_miami_pinwheel_close_up', 'real_UNL_quilt',
       'real_DIA_1990.25_o2', 'real_fowler_women_X98-16-2_front__studio',
       'real_philadelphia_museum_2001-170-1a-pma',
       'real_philadelphia_unk', 'real_miami_b',
       'real_The_Met_1972.56.1_d1', 'real_eglash',
       'real_spurlock_illinois_2011.05.0925.02',
       'real_The_Met_hb_1993.384.2'], dtype=object)
---

I wouldn't expect this to show up at all.
-
yet,

In [502]: sampling_frame.query('group not in @test_groups').group.unique()                                                                           
Out[502]: 
array(['fake_003', 'real_miami_pinwheel_close_up', 'fake_002',
       'real_DIA_1990.25_o2', 'real_The_Met_hb_1993.384.2',
       'real_The_Met_1972.56.1_d1', 'real_high_2004.166KenteCloth_o2',
       'real_miami_b', 'real_philadelphia_museum_2001-170-1a-pma',
       'real_spurlock_illinois_2011.05.0925.02', 'real_eglash',
       'real_philadelphia_unk'], dtype=object)

       has no fake_001
--
this preserves it
In [504]: foo = sampling_frame.query('group not in @test_groups').index                                                                              

In [505]: sampling_frame.loc[foo].group.unique()                                                                                                     
Out[505]: 
array(['fake_003', 'real_miami_pinwheel_close_up', 'fake_002',
       'real_DIA_1990.25_o2', 'real_The_Met_hb_1993.384.2',
       'real_The_Met_1972.56.1_d1', 'real_high_2004.166KenteCloth_o2',
       'real_miami_b', 'real_philadelphia_museum_2001-170-1a-pma',
       'real_spurlock_illinois_2011.05.0925.02', 'real_eglash',
       'real_philadelphia_unk'], dtype=object)
----
so it looks like the StratifiedShuffleSplit function is somehow
sampling into the test group
--
okay I think it returns indices into the data passed in, not the value,
so we need to use iloc instead of loc.
actually, I think we might need to double index, present a restricted 
set to the StratifiedShuffleSplit and then index into that. I thought it 
would index into what was presented byt , well, I can prsent the
restricted index values and then use iloc to use that, though.
--
that doesn't work
defintely; somethign weird is happening. Will have to get back on this tmmrw.
It might be easier here to genrate a new data frame from the non_group and
go off of that.
^
---
3/10

okay, so I ended up constructing copy of the data that only had 
non test group samples. I'm not sure why working off the original dataset
would still give me incides that occured in the test group but this removes
that block, ugh.

The files have been copied out to processed and can be used in the oc-svm repo.